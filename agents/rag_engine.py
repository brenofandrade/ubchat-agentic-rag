"""
Motor RAG (Retrieval-Augmented Generation) usando Ollama e Pinecone.
"""

import logging
import re
from typing import List, Optional, Dict, Any
from dataclasses import dataclass

from pinecone import Pinecone
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_pinecone import PineconeVectorStore
from openai import OpenAI

try:
    from pinecone_text.hybrid import hybrid_convex_scale
    from pinecone_text.sparse import SpladeSparseEncoder
except ImportError:  # pragma: no cover - fallback when library is missing
    hybrid_convex_scale = None
    SpladeSparseEncoder = None

# Importa configura√ß√µes
from config import (
    OLLAMA_BASE_URL,
    GENERATION_MODEL,
    EMBEDDING_MODEL,
    PINECONE_API_KEY,
    PINECONE_INDEX_NAME,
    DEFAULT_NAMESPACE,
    RETRIEVAL_K,
    OPENAI_KEY,
    RERANK_METHOD_DEFAULT,
    RERANK_TOP_K_DEFAULT,
    CROSS_ENCODER_MODEL,
    RERANK_BATCH_SIZE,
    MAX_HISTORY,
    ENABLE_HYBRID_SEARCH,
    HYBRID_ALPHA,
    SPLADE_MODEL
)

logger = logging.getLogger(__name__)


@dataclass
class RetrievedDocument:
    """Documento recuperado do vector store."""
    content: str
    metadata: Dict[str, Any]
    score: float

    def __repr__(self):
        return f"RetrievedDocument(score={self.score:.3f}, content='{self.content[:50]}...')"


def extract_document_identifiers(query: str) -> List[str]:
    """
    Extrai identificadores de documentos da query.

    Detecta padr√µes como:
    - MAN-XXX (Manuais)
    - NR-XXX (Normas Regulamentadoras)
    - PROC-XXX (Procedimentos)
    - DOC-XXX (Documentos gerais)
    - ISO-XXX (Normas ISO)
    - E outros padr√µes alfanum√©ricos similares

    Args:
        query: Consulta do usu√°rio

    Returns:
        Lista de identificadores encontrados
    """
    # Padr√µes comuns de identificadores de documentos
    patterns = [
        r'\b([A-Z]{2,6}-\d{2,6})\b',  # Padr√£o geral: XXX-NNN ou XXXXXX-NNNNNN
        r'\b([A-Z]{2,6}\s*\d{2,6})\b',  # Padr√£o com espa√ßo: XXX NNN
        r'\b(NR\s*-?\s*\d{1,3})\b',  # Normas Regulamentadoras espec√≠ficas
    ]

    identifiers = []
    for pattern in patterns:
        matches = re.finditer(pattern, query, re.IGNORECASE)
        for match in matches:
            identifier = match.group(1).strip()
            # Normaliza o identificador (remove espa√ßos extras, normaliza h√≠fens)
            identifier = re.sub(r'\s+', '', identifier)  # Remove espa√ßos
            identifier = re.sub(r'([A-Z]+)(\d+)', r'\1-\2', identifier, flags=re.IGNORECASE)  # Adiciona h√≠fen se n√£o tiver
            identifier = identifier.upper()
            if identifier not in identifiers:
                identifiers.append(identifier)

    return identifiers


def build_metadata_filters(identifiers: List[str]) -> Optional[Dict[str, Any]]:
    """
    Constr√≥i filtros de metadados do Pinecone baseado em identificadores.

    Args:
        identifiers: Lista de identificadores de documentos

    Returns:
        Dicion√°rio de filtros do Pinecone ou None se vazio
    """
    if not identifiers:
        return None

    # Pinecone usa sintaxe MongoDB para filtros
    # Vamos procurar em v√°rios campos de metadados poss√≠veis
    if len(identifiers) == 1:
        # Para um √∫nico identificador, busca exata ou parcial
        identifier = identifiers[0]
        return {
            "$or": [
                {"document_id": {"$eq": identifier}},
                {"doc_id": {"$eq": identifier}},
                {"id": {"$eq": identifier}},
                {"source": {"$eq": identifier}},
                {"title": {"$eq": identifier}},
                {"name": {"$eq": identifier}},
                # Busca parcial usando regex (se suportado pelo √≠ndice)
                {"document_id": {"$in": [identifier]}},
                {"source": {"$in": [identifier]}},
            ]
        }
    else:
        # Para m√∫ltiplos identificadores, busca por qualquer um deles
        or_conditions = []
        for identifier in identifiers:
            or_conditions.extend([
                {"document_id": {"$eq": identifier}},
                {"doc_id": {"$eq": identifier}},
                {"id": {"$eq": identifier}},
                {"source": {"$eq": identifier}},
                {"title": {"$eq": identifier}},
                {"name": {"$eq": identifier}},
            ])
        return {"$or": or_conditions}


class RAGEngine:
    """
    Motor de Retrieval-Augmented Generation usando Ollama e Pinecone.

    Caracter√≠sticas:
    - Embeddings locais via Ollama
    - Vector store em nuvem via Pinecone
    - Gera√ß√£o de resposta via Ollama
    - Suporte a reranking (opcional)
    - Suporte a varia√ß√µes de consulta via OpenAI (opcional)
    """

    def __init__(
        self,
        namespace: str = DEFAULT_NAMESPACE,
        top_k: int = RETRIEVAL_K,
        rerank_method: str = RERANK_METHOD_DEFAULT,
        rerank_top_k: int = RERANK_TOP_K_DEFAULT
    ):
        """
        Inicializa o motor RAG.

        Args:
            namespace: Namespace do Pinecone para isolar documentos
            top_k: N√∫mero de documentos a recuperar
            rerank_method: M√©todo de reranking ('none', 'cross-encoder')
            rerank_top_k: N√∫mero de documentos ap√≥s reranking (0 = usar top_k)
        """
        self.namespace = namespace
        self.top_k = top_k
        self.rerank_method = rerank_method
        self.rerank_top_k = rerank_top_k if rerank_top_k > 0 else top_k
        self.use_hybrid = ENABLE_HYBRID_SEARCH
        self.hybrid_alpha = HYBRID_ALPHA
        self.sparse_encoder = None

        logger.info(f"Inicializando RAG Engine com namespace '{namespace}'")

        if self.use_hybrid:
            if SpladeSparseEncoder is None or hybrid_convex_scale is None:
                logger.warning(
                    "Biblioteca 'pinecone-text' n√£o dispon√≠vel; desativando busca h√≠brida."
                )
                self.use_hybrid = False
            else:
                try:
                    logger.info(
                        "Carregando codificador esparso para busca h√≠brida: %s",
                        SPLADE_MODEL
                    )
                    self.sparse_encoder = SpladeSparseEncoder(model_name=SPLADE_MODEL)
                    logger.info(
                        "Busca h√≠brida ativada (alpha=%.2f)",
                        self.hybrid_alpha
                    )
                except Exception as exc:
                    logger.error(f"Erro ao carregar encoder esparso: {exc}")
                    self.use_hybrid = False

        # Inicializa cliente OpenAI (opcional, para varia√ß√µes de consulta)
        self.openai_client = None
        if OPENAI_KEY:
            try:
                self.openai_client = OpenAI(api_key=OPENAI_KEY)
                logger.info("Cliente OpenAI inicializado (para varia√ß√µes de consulta)")
            except Exception as e:
                logger.warning(f"Falha ao inicializar OpenAI: {e}")
        else:
            logger.info("OPENAI_API_KEY n√£o configurada; usando fallback para varia√ß√µes")

        # Inicializa Pinecone
        logger.info(f"Conectando ao Pinecone index '{PINECONE_INDEX_NAME}'")
        self.pc = Pinecone(api_key=PINECONE_API_KEY)
        self.index = self.pc.Index(PINECONE_INDEX_NAME)
        self._verify_pinecone_connection()

        # Inicializa LLM Ollama para gera√ß√£o
        logger.info(f"Inicializando Ollama LLM: {GENERATION_MODEL}")
        self.llm = ChatOllama(
            model=GENERATION_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=0
        )

        # Inicializa embeddings Ollama
        logger.info(f"Inicializando Ollama Embeddings: {EMBEDDING_MODEL}")
        self.embeddings = OllamaEmbeddings(
            model=EMBEDDING_MODEL,
            base_url=OLLAMA_BASE_URL
        )

        # Cria a conex√£o com o Vectorstore
        self.vectorstore = PineconeVectorStore(
            index=self.index,
            embedding=self.embeddings,
            namespace=self.namespace
        )

        # Inicializa reranker se necess√°rio
        self.reranker = None
        if self.rerank_method == "cross-encoder":
            try:
                from sentence_transformers import CrossEncoder
                logger.info(f"Carregando Cross-Encoder: {CROSS_ENCODER_MODEL}")
                self.reranker = CrossEncoder(CROSS_ENCODER_MODEL)
            except Exception as e:
                logger.error(f"Erro ao carregar Cross-Encoder: {e}")
                self.rerank_method = "none"

        logger.info("‚úì RAG Engine inicializado com sucesso")

    def retrieve(
        self,
        query: str,
        top_k: Optional[int] = None,
        metadata_filters: Optional[Dict[str, Any]] = None,
        auto_detect_identifiers: bool = True
    ) -> List[RetrievedDocument]:
        """
        Recupera documentos relevantes do vector store.

        Args:
            query: Consulta do usu√°rio
            top_k: N√∫mero de documentos a recuperar (sobrescreve padr√£o)
            metadata_filters: Filtros de metadados para o Pinecone (opcional)
            auto_detect_identifiers: Se True, detecta automaticamente identificadores
                                      de documentos na query (ex: MAN-297, NR-013)

        Returns:
            Lista de documentos recuperados
        """
        k = top_k if top_k is not None else self.top_k

        # Detecta identificadores de documentos automaticamente se habilitado
        if auto_detect_identifiers and metadata_filters is None:
            identifiers = extract_document_identifiers(query)
            if identifiers:
                metadata_filters = build_metadata_filters(identifiers)
                logger.info(
                    f"üîç Identificadores detectados: {identifiers} - aplicando busca por metadados"
                )

        if metadata_filters:
            logger.info(
                f"Recuperando {k} documentos com filtros de metadados para query: '{query[:100]}...'"
            )
        else:
            logger.info(f"Recuperando {k} documentos para query: '{query[:100]}...'")

        try:
            if self.use_hybrid and self.sparse_encoder:
                documents = self._hybrid_retrieve(query, k, metadata_filters)
                if not documents:
                    logger.warning(
                        "Busca h√≠brida n√£o retornou resultados; usando busca densa como fallback."
                    )
                    documents = self._dense_retrieve(query, k, metadata_filters)
            else:
                documents = self._dense_retrieve(query, k, metadata_filters)

            # Aplica reranking se configurado
            if self.rerank_method != "none" and len(documents) > 0:
                documents = self._rerank(query, documents)

            return documents

        except Exception as e:
            logger.error(f"Erro ao recuperar documentos: {e}")
            return []

    def _dense_retrieve(
        self,
        query: str,
        k: int,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> List[RetrievedDocument]:
        """
        Executa recupera√ß√£o baseada apenas em embeddings densos.

        Args:
            query: Consulta do usu√°rio
            k: N√∫mero de documentos a recuperar
            metadata_filters: Filtros de metadados para o Pinecone

        Returns:
            Lista de documentos recuperados
        """
        search_kwargs = {
            "k": k,
            "namespace": self.namespace
        }

        # Adiciona filtros de metadados se fornecidos
        if metadata_filters:
            search_kwargs["filter"] = metadata_filters

        results = self.vectorstore.similarity_search_with_score(
            query=query,
            **search_kwargs
        )

        documents = [
            RetrievedDocument(
                content=doc.page_content,
                metadata=doc.metadata,
                score=score
            )
            for doc, score in results
        ]

        filter_info = " (com filtros de metadados)" if metadata_filters else ""
        logger.info(f"‚úì Recuperados {len(documents)} documentos (busca densa{filter_info})")
        return documents

    def _hybrid_retrieve(
        self,
        query: str,
        k: int,
        metadata_filters: Optional[Dict[str, Any]] = None
    ) -> List[RetrievedDocument]:
        """
        Executa recupera√ß√£o h√≠brida combinando sinais densos e esparsos.

        Args:
            query: Consulta do usu√°rio
            k: N√∫mero de documentos a recuperar
            metadata_filters: Filtros de metadados para o Pinecone

        Returns:
            Lista de documentos recuperados
        """
        try:
            dense_vector = self.embeddings.embed_query(query)
        except Exception as exc:
            logger.error(f"Erro ao gerar embedding para busca h√≠brida: {exc}")
            return []

        try:
            sparse_vector = self.sparse_encoder.encode_queries([query])[0]
        except Exception as exc:
            logger.error(f"Erro ao gerar vetor esparso para busca h√≠brida: {exc}")
            return []

        if hybrid_convex_scale:
            try:
                dense_vector, sparse_vector = hybrid_convex_scale(
                    dense_vector,
                    sparse_vector,
                    self.hybrid_alpha
                )
            except Exception as exc:
                logger.error(f"Erro ao aplicar escala h√≠brida: {exc}")
                return []

        # Prepara par√¢metros da query
        query_params = {
            "vector": dense_vector,
            "sparse_vector": sparse_vector,
            "namespace": self.namespace,
            "top_k": k,
            "include_metadata": True
        }

        # Adiciona filtros de metadados se fornecidos
        if metadata_filters:
            query_params["filter"] = metadata_filters

        try:
            response = self.index.query(**query_params)
        except Exception as exc:
            logger.error(f"Erro ao consultar Pinecone (busca h√≠brida): {exc}")
            return []

        matches = getattr(response, "matches", []) or []
        documents: List[RetrievedDocument] = []

        for match in matches:
            metadata = getattr(match, "metadata", None)
            if metadata is None and isinstance(match, dict):
                metadata = match.get("metadata", {})
            metadata = metadata or {}

            content = (
                metadata.get("text")
                or metadata.get("page_content")
                or metadata.get("content")
                or metadata.get("chunk")
                or metadata.get("body")
                or ""
            )

            if not content and isinstance(match, dict):
                content = match.get("text", "")

            score = getattr(match, "score", None)
            if score is None and isinstance(match, dict):
                score = match.get("score", 0.0)

            try:
                score_value = float(score) if score is not None else 0.0
            except (TypeError, ValueError):
                score_value = 0.0

            documents.append(
                RetrievedDocument(
                    content=content,
                    metadata=metadata,
                    score=score_value
                )
            )

        filter_info = " (com filtros de metadados)" if metadata_filters else ""
        logger.info(f"‚úì Recuperados {len(documents)} documentos (busca h√≠brida{filter_info})")
        return documents

    def _verify_pinecone_connection(self) -> None:
        """Verifica se a conex√£o com o Pinecone est√° operacional."""
        try:
            stats = self.index.describe_index_stats(namespace=self.namespace)
            dimension = None
            if isinstance(stats, dict):
                dimension = stats.get("dimension")
            else:
                dimension = getattr(stats, "dimension", None)
            logger.info(
                "‚úì Conex√£o com Pinecone verificada (dimens√£o: %s)",
                dimension if dimension is not None else "desconhecida"
            )
        except Exception as exc:
            logger.error(f"Falha ao verificar conex√£o com Pinecone: {exc}")
            raise RuntimeError("N√£o foi poss√≠vel verificar a conex√£o com o Pinecone.") from exc

    def _rerank(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """
        Reordena documentos usando cross-encoder.

        Args:
            query: Consulta original
            documents: Documentos a reordenar

        Returns:
            Documentos reordenados
        """
        if not self.reranker:
            logger.warning("Reranker n√£o dispon√≠vel, pulando reranking")
            return documents

        logger.info(f"Aplicando reranking com {self.rerank_method}")

        try:
            # Prepara pares (query, documento)
            pairs = [(query, doc.content) for doc in documents]

            # Calcula scores de relev√¢ncia
            scores = self.reranker.predict(pairs, batch_size=RERANK_BATCH_SIZE)

            # Atualiza scores e reordena
            for doc, score in zip(documents, scores):
                doc.score = float(score)

            # Ordena por score (maior primeiro) e pega top_k
            reranked = sorted(documents, key=lambda x: x.score, reverse=True)
            reranked = reranked[:self.rerank_top_k]

            logger.info(f"‚úì Reranking conclu√≠do: {len(reranked)} documentos")
            return reranked

        except Exception as e:
            logger.error(f"Erro no reranking: {e}")
            return documents

    def generate_answer(
        self,
        query: str,
        documents: List[RetrievedDocument],
        chat_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        Gera resposta usando documentos recuperados.

        Args:
            query: Pergunta do usu√°rio
            documents: Documentos relevantes
            chat_history: Hist√≥rico de conversa (opcional)

        Returns:
            Resposta gerada
        """
        logger.info(f"Gerando resposta com {len(documents)} documentos")

        # Prepara contexto dos documentos
        context = "\n\n".join([
            f"Documento {i+1} (score: {doc.score:.3f}):\n{doc.content}"
            for i, doc in enumerate(documents)
        ])

        # Prepara hist√≥rico de conversa
        history_text = ""
        if chat_history:
            history_items = chat_history[-MAX_HISTORY:]  # Limita hist√≥rico
            history_text = "\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in history_items
            ])
            history_text = f"\n\nHist√≥rico da conversa:\n{history_text}\n"

        # Monta prompt
        prompt = f"""Voc√™ √© um assistente prestativo que responde perguntas com base em documentos fornecidos.

{history_text}
Contexto dos documentos:
{context}

Pergunta do usu√°rio: {query}

Instru√ß√µes:
1. Responda APENAS com base nos documentos fornecidos
2. Se os documentos n√£o contiverem informa√ß√£o suficiente, diga claramente
3. Cite os documentos quando relevante (ex: "De acordo com o Documento 1...")
4. Seja objetivo e direto
5. Responda em portugu√™s brasileiro

Resposta:"""

        try:
            # Gera resposta via Ollama
            response = self.llm.invoke(prompt)

            # Extrai texto da resposta
            if hasattr(response, 'content'):
                answer = response.content
            else:
                answer = str(response)

            logger.info("‚úì Resposta gerada com sucesso")
            return answer

        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}")
            return f"Desculpe, ocorreu um erro ao gerar a resposta: {str(e)}"

    def query(
        self,
        query: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        top_k: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Pipeline completo RAG: recupera√ß√£o + gera√ß√£o.

        Args:
            query: Pergunta do usu√°rio
            chat_history: Hist√≥rico de conversa
            top_k: N√∫mero de documentos a recuperar

        Returns:
            Dicion√°rio com resposta e metadados
        """
        logger.info(f"Executando query RAG: '{query[:100]}...'")

        # Recupera documentos
        documents = self.retrieve(query, top_k=top_k)

        if not documents:
            return {
                "answer": "Desculpe, n√£o encontrei documentos relevantes para sua pergunta.",
                "documents": [],
                "metadata": {
                    "retrieved_count": 0,
                    "generation_model": GENERATION_MODEL,
                    "embedding_model": EMBEDDING_MODEL
                }
            }

        # Gera resposta
        answer = self.generate_answer(query, documents, chat_history)

        # Retorna resultado completo
        return {
            "answer": answer,
            "documents": [
                {
                    "content": doc.content,
                    "metadata": doc.metadata,
                    "score": doc.score
                }
                for doc in documents
            ],
            "metadata": {
                "retrieved_count": len(documents),
                "generation_model": GENERATION_MODEL,
                "embedding_model": EMBEDDING_MODEL,
                "namespace": self.namespace,
                "rerank_method": self.rerank_method
            }
        }


# Inst√¢ncia global (singleton)
_rag_engine_instance = None


def get_rag_engine(
    namespace: str = DEFAULT_NAMESPACE,
    force_new: bool = False
) -> RAGEngine:
    """
    Retorna inst√¢ncia singleton do RAG Engine.

    Args:
        namespace: Namespace do Pinecone
        force_new: For√ßa cria√ß√£o de nova inst√¢ncia

    Returns:
        Inst√¢ncia do RAG Engine
    """
    global _rag_engine_instance

    if _rag_engine_instance is None or force_new:
        _rag_engine_instance = RAGEngine(namespace=namespace)

    return _rag_engine_instance
