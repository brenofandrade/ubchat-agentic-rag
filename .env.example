# ============================================
# UBChat Agentic RAG - Environment Variables
# ============================================
# Copy this file to .env and configure your values

# --- Logging Configuration ---
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

# --- Ollama Configuration ---
# URL base do servidor Ollama (local ou remoto)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo para geração de respostas
GENERATION_MODEL=llama3.2:latest

# Modelo para embeddings
EMBEDDING_MODEL=mxbai-embed-large:latest

# --- Pinecone Configuration (Vector Database) ---
# Chave de API do Pinecone (obrigatório)
PINECONE_API_KEY_DSUNIBLU=your-pinecone-api-key-here

# Nome do índice no Pinecone (obrigatório)
PINECONE_INDEX=your-index-name
# Ou use esta variável alternativa:
# PINECONE_INDEX_NAME=your-index-name

# Namespace padrão para organizar documentos
PINECONE_NAMESPACE=default

# --- Retrieval Configuration ---
# Número de documentos a recuperar por query
RETRIEVAL_K=2

# --- Reranking Configuration ---
# Método de reranking: "none", "cross-encoder"
RERANK_METHOD_DEFAULT=none

# Número de documentos após reranking (0 = usar RETRIEVAL_K)
RERANK_TOP_K_DEFAULT=0

# Modelo de cross-encoder para reranking
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Tamanho do batch para reranking
RERANK_BATCH_SIZE=16

# --- Chat History Configuration ---
# Número máximo de mensagens no histórico
MAX_HISTORY=10

# TTL (Time To Live) para sessões em segundos
TTL_SETUP=1200

# --- Optional: OpenAI for Query Variations (fallback) ---
# Se não configurado, usa método rule-based
OPENAI_API_KEY=your-openai-api-key-here

# --- Query Router Configuration ---
# Provider: "ollama", "openai", ou "anthropic"
LLM_PROVIDER=ollama

# Modelo do LLM (depende do provider)
LLM_MODEL=llama3.2:latest

# --- Streamlit UI Configuration ---
# Versão da aplicação
APP_VERSION=1.0.0

# URL da API de autenticação (opcional, deixe vazio se não usar)
API_URL=

# Token de autenticação para API externa (opcional)
AUTH_TOKEN=

# URL do backend (se não definido, usa http://localhost:BACKEND_PORT)
BACKEND_URL=http://localhost:8000

# Porta do backend
BACKEND_PORT=8000

# ID do POD (para RunPod, deixe vazio se local)
POD_ID=
