# UBChat Agentic RAG

Sistema de RAG (Retrieval-Augmented Generation) com roteamento inteligente de consultas usando **Ollama** (modelos locais) e **Pinecone** (banco de dados vetorizado em nuvem).

## üöÄ Caracter√≠sticas

- **Modelos Locais**: Usa Ollama para executar LLMs localmente (economia de custos)
- **Vector Store em Nuvem**: Pinecone para armazenamento escal√°vel de embeddings
- **Roteamento Inteligente**: Decide automaticamente entre RAG, resposta direta ou pedido de esclarecimento
- **Reranking Opcional**: Cross-encoder para melhorar relev√¢ncia dos documentos
- **API REST**: Endpoints Flask para f√°cil integra√ß√£o
- **Hist√≥rico de Conversa**: Suporte a contexto conversacional

## üìã Pr√©-requisitos

### 1. Ollama

Instale o Ollama seguindo as instru√ß√µes em [ollama.ai](https://ollama.ai)

Baixe os modelos necess√°rios:

```bash
# Modelo para gera√ß√£o de respostas
ollama pull llama3.2:latest

# Modelo para embeddings
ollama pull mxbai-embed-large:latest
```

Verifique se o Ollama est√° rodando:

```bash
ollama list
curl http://localhost:11434/api/tags
```

### 2. Pinecone

1. Crie uma conta em [Pinecone](https://www.pinecone.io/)
2. Crie um √≠ndice com as seguintes configura√ß√µes:
   - **Dimens√µes**: 1024 (para `mxbai-embed-large`)
   - **M√©trica**: cosine
   - **Cloud**: Escolha a regi√£o mais pr√≥xima

3. Obtenha sua API Key no dashboard

## üõ†Ô∏è Instala√ß√£o

### 1. Clone o reposit√≥rio

```bash
git clone <repository-url>
cd ubchat-agentic-rag
```

### 2. Crie ambiente virtual

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

### 3. Instale depend√™ncias

```bash
pip install -r requirements.txt
```

### 4. Configure vari√°veis de ambiente

```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas configura√ß√µes:

```bash
# Pinecone (obrigat√≥rio)
PINECONE_API_KEY_DSUNIBLU=your-pinecone-api-key
PINECONE_INDEX=your-index-name

# Ollama (ajuste se necess√°rio)
OLLAMA_BASE_URL=http://localhost:11434
GENERATION_MODEL=llama3.2:latest
EMBEDDING_MODEL=mxbai-embed-large:latest
```

## üöÄ Executando o Sistema

### Iniciar o backend

```bash
python main.py
```

O servidor backend estar√° dispon√≠vel em `http://localhost:8000`

### Iniciar a interface web (Streamlit)

Em outro terminal, execute:

```bash
streamlit run ui_app.py
```

A interface web estar√° dispon√≠vel em `http://localhost:8501`

**Nota**: O backend deve estar rodando antes de iniciar a interface web.

### Health Check

```bash
curl http://localhost:8000/health
```

Resposta esperada:

```json
{
  "status": "ok",
  "provider": "ollama",
  "model": "llama3.2:latest",
  "pinecone_index": "your-index-name",
  "namespace": "default"
}
```

## üì° API Endpoints

### 1. Roteamento de Consulta

**POST** `/route-query`

Decide a melhor estrat√©gia para responder uma pergunta.

```bash
curl -X POST http://localhost:8000/route-query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Qual √© a pol√≠tica de f√©rias da empresa?",
    "context": "Preciso saber sobre benef√≠cios"
  }'
```

Resposta:

```json
{
  "route": "rag",
  "confidence": 0.95,
  "reasoning": "Pergunta sobre pol√≠tica interna da empresa",
  "suggested_documents": ["company_policies"]
}
```

### 2. Query RAG Completa

**POST** `/rag/query`

Recupera documentos e gera resposta.

```bash
curl -X POST http://localhost:8000/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como funciona o processo de reembolso?",
    "top_k": 3,
    "chat_history": [
      {"role": "user", "content": "Oi"},
      {"role": "assistant", "content": "Ol√°! Como posso ajudar?"}
    ]
  }'
```

Resposta:

```json
{
  "answer": "De acordo com o Documento 1, o processo de reembolso...",
  "documents": [
    {
      "content": "Processo de Reembolso: ...",
      "metadata": {"source": "manual.pdf", "page": 5},
      "score": 0.92
    }
  ],
  "metadata": {
    "retrieved_count": 3,
    "generation_model": "llama3.2:latest",
    "embedding_model": "mxbai-embed-large:latest",
    "namespace": "default"
  }
}
```

### 3. Apenas Recupera√ß√£o de Documentos

**POST** `/rag/retrieve`

Recupera documentos sem gerar resposta.

```bash
curl -X POST http://localhost:8000/rag/retrieve \
  -H "Content-Type: application/json" \
  -d '{
    "question": "pol√≠tica de f√©rias",
    "top_k": 5
  }'
```

### 4. Chat Completo (Roteamento + RAG)

**POST** `/chat`

Endpoint completo que decide automaticamente a melhor estrat√©gia.

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como funciona fotoss√≠ntese?",
    "chat_history": []
  }'
```

## üíª Interface Web (Streamlit)

A aplica√ß√£o inclui uma interface web moderna e intuitiva constru√≠da com Streamlit.

### Funcionalidades da Interface

- **Chat Interativo**: Interface de conversa√ß√£o natural
- **Hist√≥rico de Mensagens**: Mant√©m contexto da conversa
- **Exibi√ß√£o de Fontes**: Mostra documentos que foram usados para gerar a resposta
- **Verifica√ß√£o de Servidor**: Bot√£o para verificar status do backend
- **Feedback**: Sistema de avalia√ß√£o de respostas
- **Nova Conversa**: Bot√£o para reiniciar a sess√£o
- **Autentica√ß√£o** (opcional): Sistema de login para controlar acesso

### Configura√ß√£o da Interface

As configura√ß√µes da interface s√£o feitas atrav√©s de vari√°veis de ambiente no arquivo `.env`:

```bash
# Configura√ß√µes da Interface Streamlit
APP_VERSION=1.0.0                    # Vers√£o da aplica√ß√£o
BACKEND_URL=http://localhost:8000    # URL do backend
BACKEND_PORT=8000                    # Porta do backend
API_URL=                             # URL da API de autentica√ß√£o (opcional)
AUTH_TOKEN=                          # Token de autentica√ß√£o (opcional)
POD_ID=                              # ID do POD para RunPod (opcional)
```

### Monitoramento

A interface registra automaticamente:
- **Hist√≥rico de perguntas**: `monitoramento/history.log`
- **Erros**: `monitoramento/erros.log`
- **Feedback dos usu√°rios**: `monitoramento/feedback.log`

Esses logs incluem:
- Timestamp
- Sess√£o ID
- Pergunta e resposta
- Lat√™ncia
- Modo de opera√ß√£o (RAG, direto, etc.)
- Informa√ß√µes de uso

## üèóÔ∏è Arquitetura

```
ubchat-agentic-rag/
‚îú‚îÄ‚îÄ main.py                    # API Flask (Backend)
‚îú‚îÄ‚îÄ ui_app.py                  # Interface Streamlit (Frontend)
‚îú‚îÄ‚îÄ config.py                  # Configura√ß√µes centralizadas
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias
‚îú‚îÄ‚îÄ .env.example              # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ query_router.py       # Roteamento de consultas
‚îÇ   ‚îî‚îÄ‚îÄ rag_engine.py         # Motor RAG (Ollama + Pinecone)
‚îú‚îÄ‚îÄ monitoramento/            # Logs e monitoramento (criado automaticamente)
‚îÇ   ‚îú‚îÄ‚îÄ history.log          # Hist√≥rico de intera√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ erros.log            # Log de erros
‚îÇ   ‚îî‚îÄ‚îÄ feedback.log         # Feedback dos usu√°rios
‚îî‚îÄ‚îÄ README.md
```

## üîß Configura√ß√£o Avan√ßada

### Reranking

Ative reranking para melhorar relev√¢ncia:

```bash
# .env
RERANK_METHOD_DEFAULT=cross-encoder
RERANK_TOP_K_DEFAULT=3
CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
```

### Namespaces

Use namespaces para isolar documentos por contexto:

```bash
curl -X POST http://localhost:8000/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "pergunta",
    "namespace": "politicas-rh"
  }'
```

### Modelos Alternativos

Troque os modelos no `.env`:

```bash
# Para respostas mais r√°pidas (menor qualidade)
GENERATION_MODEL=llama3.2:1b

# Para melhor qualidade (mais lento)
GENERATION_MODEL=llama3.1:70b

# Embeddings alternativos
EMBEDDING_MODEL=nomic-embed-text:latest
```

**IMPORTANTE**: Ajuste as dimens√µes do √≠ndice Pinecone de acordo com o modelo de embedding escolhido.

## üß™ Testes

```bash
# Testar roteamento
python test_query_router.py

# Testar API
python teste_api.py
```

## üìä Monitoramento

### Logs

Configure n√≠vel de log no `.env`:

```bash
LOG_LEVEL=DEBUG  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

### M√©tricas

O sistema loga automaticamente:
- Tempo de recupera√ß√£o
- N√∫mero de documentos recuperados
- Scores de relev√¢ncia
- Erros e fallbacks

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìù Licen√ßa

Este projeto est√° sob a licen√ßa MIT.

## üÜò Troubleshooting

### Ollama n√£o conecta

```bash
# Verifique se o servi√ßo est√° rodando
ollama list

# Reinicie o Ollama
ollama serve
```

### Pinecone timeout

- Verifique sua API key
- Confirme que o √≠ndice existe
- Verifique conectividade com a internet

### Embeddings com dimens√£o errada

Certifique-se de que as dimens√µes do √≠ndice Pinecone correspondem ao modelo:
- `mxbai-embed-large`: 1024 dimens√µes
- `nomic-embed-text`: 768 dimens√µes
- `all-MiniLM-L6-v2`: 384 dimens√µes

### Modelo n√£o encontrado

```bash
# Liste modelos instalados
ollama list

# Baixe o modelo necess√°rio
ollama pull llama3.2:latest
```

## üìö Documenta√ß√£o Adicional

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [LangChain Documentation](https://python.langchain.com/)
- [Query Router README](QUERY_ROUTER_README.md)